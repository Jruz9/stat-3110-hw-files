---
title: "stat-3110-Homwork9"
author: "Jose Cruz"
date: "10/30/2021"
output: word_document
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

# Homework 9

## Question 1 : For the prostate data in package faraway, fit a linear regression model with lpsa as the response variable and the other variables as predictors.

### (a) Check the constant variance assumption by plotting standardized residual vs fitted values;

```{r}

library(faraway)
data(prostate,package = "faraway")

prostateModel <-lm(lpsa~., data = prostate)

summary(prostateModel)


plot(fitted(prostateModel), residuals(prostateModel), xlab="Fitted", ylab="Residuals")
plot(prostateModel)




```

In our plots our data shows that our points are above and below our y- axis at 0, showing that our variance is constant and symmetrical.

### (b) Check the normality assumption by using QQ plot;

```{r}
#Nomrallity  assumption with qq plot


qqnorm(residuals(prostateModel), ylab="Residuals")
#qqline(residuals(prostateModel)) doesnt work for some reason.

```

Using a QQ plot to check our normality assumption, we see that since out points follow a straight line, we can assume that it follows a normal distribution.

### (c) Check for any leverage points and generate a half normal plot of hat values;

```{r}
#calculate the leverage points 

hatv <- hatvalues(prostateModel) ## Calculate leverages
head(hatv)

hatv[which(hatv>2*mean(hatv))]
```

Checking our leverage points we can see that we have 5 leverage points significantly different from the rest in our dataset.

```{r}

# generates the half normal plot for leverage points
halfnorm(hatv, nlab=4, labs=row.names(prostate), ylab="Leverages")
```

Using a half normal chart, we can see that the five leverage points we calculated are ploted and show us how much they can effect our model.

### (d) Check for any outliers in the data set;

```{r}
# calculate outlier in our data set
#prostateModel.head()
#plot(y~x, rbind(testdata, p1))

#plot(prostate$lcavol,prostate$lweight,xlab="log(lcavol)", ylab="log(lweight)")




stud <- rstudent(prostateModel)
max(abs(stud))

#lmod <- lm(lcavol~lweight, data=prostate)
#abline(lmod)
#range(rstudent(lmod))

```

In our calculation we found that 2.61998 was a significant outlier in our data set.

### (e) Check for any influential data points.

```{r pressure, echo=FALSE}
cook <- cooks.distance(prostateModel)

halfnorm(cook, nlab=3, labs=row.names(prostate), ylab="Cook statistics")
```

Using half normal to find any influential points, we found though using cook statistics that there are 3 big possible influential points in our data.

## Question 2. (problem 5 on page 110) For the prostate data, fit a model with lpsa as the response and the other variables as predictors.

### (A) Compute and comment on the condition numbers;

```{r}
#Calculate the conditional numbers(co linearity)

summary(prostateModel)



```

#### Conditional numbers

```{r}

x <- model.matrix(prostateModel)[, -1]
e <- eigen(t(x)%*%x)
e$val

```

Here we have the calculated conditional numbers

#### Kappa calculations

```{r}
kappa <- sqrt(e$val[1]/e$val)
kappa>30
```

Calculating our conditional numbers and trying to find our which conditional numbers have a k greater than 30 we see that the problems in our dataset are being caused by more than 1 linear combination.

### (B) Compute and comment on the correlations between the predictors;

```{r}
round(cor(prostate[, -8]), 2)
```

From looking at our correlation matrix we see that: - lcavol correlates with svi (0.54), lcp(0.68) and lpsa (0.73) - svi correlates with lcp(0.67)

From these correlation we can see that lacvol and svi correlated with the other predictors the higher than the rest.

### (C) Compute the variance inflation factors (VIFs).

```{r}

Rj2 <- summary(lm(x[, 1]~x[, -1]))$r.squared
1/(1-Rj2)

```

Computing our VIF we find that we get a value of 2.054115 for our lcavol .

```{r}

vif(prostateModel)


```

Computing the rest of the of the VIF in our model we find that there is a above average amount of inflation in our variation for our data set.
